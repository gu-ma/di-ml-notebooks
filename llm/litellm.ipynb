{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df54c017-ce9d-4712-a34f-9f5efa9ef1f5",
   "metadata": {},
   "source": [
    "Install litellm\n",
    "\n",
    "https://docs.litellm.ai/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae653d1-cf4b-4bd3-afc8-ca9ff84a2118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d1b5a-13b8-4ce7-b696-b121d5041156",
   "metadata": {},
   "source": [
    "You need to create a REPLICATE API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f1cbc-ea73-430b-82a1-78841aeefb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "## set ENV variables\n",
    "os.environ[\"REPLICATE_API_KEY\"] = \"[YOUR_TOKEN]\"\n",
    "\n",
    "# define messages\n",
    "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "\n",
    "# replicate call\n",
    "response = litellm.completion(\n",
    "    model=\"replicate/anthropic/claude-3.5-haiku\", \n",
    "    messages=messages    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48efadb-987b-4ee3-9a66-426a4a5ad16a",
   "metadata": {},
   "source": [
    "Inspect response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87fdd0-2ebe-421f-9737-0720870e44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e42496-4f00-46e5-a55a-859f93e5ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in response.model_dump().keys():\n",
    "    value = response.get(key)\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b5707-586e-4b2b-9b1f-b1ac93ea4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in response.choices[0].model_dump().keys():\n",
    "    value = response.choices[0].get(key)\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcba5bc-e60d-40a1-b24e-9c049e714bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c454455-984f-409a-9b26-b893e11f300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690c982-d5a3-41de-b179-7dc46f4fd2fc",
   "metadata": {},
   "source": [
    "#### [Streaming](https://docs.litellm.ai/docs/completion/stream)\n",
    "\n",
    "LiteLLM supports streaming the model response back by passing `stream=True` as an argument to the completion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed619cb9-5aae-43a9-a9b1-ed7a24fd774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hey, how's it going? Write me a poem please\"}]\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"replicate/anthropic/claude-3.5-haiku\", \n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for part in response:\n",
    "    print(part.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbebad6-b4b3-46c5-8cc1-9c2a1b910141",
   "metadata": {},
   "source": [
    "#### Streaming helper function\n",
    "LiteLLM also exposes a helper function to rebuild the complete streaming response from the list of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a53d4d-9703-4dd6-9706-f9870340f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hey, how's it going? Write me a poem please\"}]\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"replicate/anthropic/claude-3.5-haiku\", \n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for chunk in response: \n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(litellm.stream_chunk_builder(chunks, messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168930e-2aea-4a7a-96ca-38d60940884e",
   "metadata": {},
   "source": [
    "### Gradio app\n",
    "\n",
    "Testing a simple gradio app. [Tutorial](https://www.gradio.app/guides/creating-a-chatbot-fast) / [Documentation](https://www.gradio.app/docs/gradio/chatinterface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ac134-fe35-4940-b86d-58d4f17fe827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e218233-b5c0-482b-8b6b-93d663fc9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141aa25-8961-4232-b217-844ec33125eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(message, history):\n",
    "    try:\n",
    "        \n",
    "        flattened_history = [item for sublist in history for item in sublist]\n",
    "        full_message = \" \".join(flattened_history + [message])\n",
    "        messages_litellm = [{\"role\": \"user\", \"content\": full_message}]\n",
    "        \n",
    "        partial_message = \"\"\n",
    "        \n",
    "        for chunk in litellm.completion(\n",
    "            model=\"replicate/anthropic/claude-3.5-haiku\",\n",
    "            messages=messages_litellm,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        ):\n",
    "            content = chunk['choices'][0]['delta'].get('content', '') or ''\n",
    "            partial_message += str(content)\n",
    "            yield partial_message\n",
    "\n",
    "    except Exception as e:\n",
    "        yield f\"An error occurred: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e310cb4-c471-452b-b344-b8be956a5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(\n",
    "    inference,\n",
    "    title=\"LiteLLM Chatbot\",\n",
    "    description=\"A simple chatbot using LiteLLM and Gradio\",\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c019e29-7cc7-4ab7-a4ce-e10dd83a6a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
