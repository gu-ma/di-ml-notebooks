{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82d4983",
   "metadata": {},
   "source": [
    "#### Install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75d7f7-8e10-446e-b214-a24252c79b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f021f-9660-4131-81af-91b248f24c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed7e87",
   "metadata": {},
   "source": [
    "#### List of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08223ba2-2faa-4d89-b6cc-813106291a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in gpt4all.GPT4All.list_models():\n",
    "    print(f\"{model['name']}, {model['filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc5626-529f-4f1d-ae18-13c07dd321b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gpt4all.GPT4All(\"wizardlm-13b-v1.2.Q4_0.gguf\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20153cbd-5533-450a-8d19-34d3fbc4095a",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0207d4-888f-4948-ac0f-2f9156a5ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\"The capital of Switzerland is \", max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47952adc-e37d-41bd-b0be-beee6daa133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac1ea5",
   "metadata": {},
   "source": [
    "#### Chat session example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c40466-e20f-4c3f-ad29-3ef6a0a3f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.chat_session():\n",
    "    response1 = model.generate(prompt='hello', temp=1)\n",
    "    response2 = model.generate(prompt='write me a short poem', temp=1, max_tokens=50)\n",
    "    print(model.current_chat_session)\n",
    "    sess = model.current_chat_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c255a72-6977-408f-8589-f5094b730cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in sess:\n",
    "    print(r['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac1401-9f87-45c2-a7df-c3fcf2d95c5c",
   "metadata": {},
   "source": [
    "#### Streaming example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df323df7-dd0a-40ad-b114-2d060c7de0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in model.generate(\"The sun is \", max_tokens=20, streaming=True):\n",
    "    tokens.append(token)\n",
    "    print(token, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e94ae-80be-477a-a42b-c2ecc5d5f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f04443-8bfe-4a1f-b182-a0a0c01df438",
   "metadata": {},
   "source": [
    "#### Streaming session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc99f87-1f46-4d6a-9816-f1fa329f3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.chat_session():\n",
    "    for token in model.generate(prompt='hello', temp=1, max_tokens=20, streaming=True):\n",
    "        print(token, end='')\n",
    "    for token in model.generate(prompt='write me a short poem', temp=1, streaming=True):\n",
    "        print(token, end='')\n",
    "    # print(model.current_chat_session)\n",
    "    sess = model.current_chat_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c1682-9c6a-48ff-b989-90d46f1c38d2",
   "metadata": {},
   "source": [
    "#### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca0f48-e841-45c4-8505-9b33b151f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = ''\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "prompts = ['What a beautiful day', 'Yes a good day to find a BUG in this code üêõ !']\n",
    "with model.chat_session(system_template, prompt_template):\n",
    "    for prompt in prompts:\n",
    "        response = model.generate(prompt, temp=2, max_tokens=20)\n",
    "    session = model.current_chat_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d6ec6-d6e0-4b3a-bfb3-7f8ee523a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in session[1:]:\n",
    "    print(f\"---\\n{s['role']}:\\n---\\n{s['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691237-5791-4c79-aeef-a52aec110e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = 'You\\'re a rap poet, and reply in rhymes. Your text are pretty hardcore and mean. They are full of IT joke because you\\'re a geek and you really don\\'t give a fuck'\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "prompts = ['Windows 11 is such a good OS!', 'You\\'re right, my man!']\n",
    "with model.chat_session(system_template, prompt_template):\n",
    "    for prompt in prompts:\n",
    "        print(f\"---\\n{prompt}\\n---\")\n",
    "        for token in model.generate(prompt, temp=2, streaming=True, max_tokens=100):\n",
    "            print(token, end='')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
